{
  "id" : "22cbf314-e84e-452f-865a-b16bf019d96b",
  "name" : "api_2.0_sql_statements_01efe8f8-e6a3-1b88-b331-ff4f5465bc03",
  "request" : {
    "url" : "/api/2.0/sql/statements/01efe8f8-e6a3-1b88-b331-ff4f5465bc03",
    "method" : "GET"
  },
  "response" : {
    "status" : 200,
    "body" : "{\"statement_id\":\"01efe8f8-e6a3-1b88-b331-ff4f5465bc03\",\"status\":{\"state\":\"FAILED\",\"error\":{\"error_code\":\"BAD_REQUEST\",\"message\":\"Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9348.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9348.0 (TID 26940) (10.46.106.81 executor 5): com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied; request: GET https://us-west-2-extstaging-managed-catalog-test-bucket-1.s3.us-west-2.amazonaws.com  {key=[2], key=[1], key=[19a85dee-54bc-43a2-87ab-023d0ec16013/tables/d6b63d32-6d54-4d11-a8d2-f0beaef388da/part-00000-874a9e91-96cc-4b9d-844a-17d76de01113.c000.snappy.parquet], key=[false]} Hadoop 3.3.6, aws-sdk-java/1.12.638 Linux/5.15.0-1072-aws OpenJDK_64-Bit_Server_VM/17.0.13+11-LTS java/17.0.13 scala/2.12.15 kotlin/1.9.10 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.ListObjectsV2Request; Request ID: ZM6M2258TPWT5EYG, Extended Request ID: 5fuZkN2EQQZUcbDuG/HcyatkwZhg9vo/HyfuariCCAWBFkACbaKiZ2jbsbyl/rQbVr9x7xN9PynDj00hocOg6g==, Cloud Provider: unknown, Instance ID: unknown credentials-provider: com.amazonaws.auth.AnonymousAWSCredentials credential-header: no-credential-header signature-present: false (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: ZM6M2258TPWT5EYG; S3 Extended Request ID: 5fuZkN2EQQZUcbDuG/HcyatkwZhg9vo/HyfuariCCAWBFkACbaKiZ2jbsbyl/rQbVr9x7xN9PynDj00hocOg6g==; Proxy: null), S3 Extended Request ID: 5fuZkN2EQQZUcbDuG/HcyatkwZhg9vo/HyfuariCCAWBFkACbaKiZ2jbsbyl/rQbVr9x7xN9PynDj00hocOg6g==\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\\n\\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\\n\\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\\n\\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\\n\\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5520)\\n\\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5467)\\n\\tat com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1001)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.listObjectsV2(EnforcingDatabricksS3Client.scala:213)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$listObjects$11(S3AFileSystem.java:2722)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:435)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:394)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:2711)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.listOneKeyWithPrefix(S3AFileSystem.java:6084)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getExistingFileSizeByListing(S3AFileSystem.java:6033)\\n\\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:1845)\\n\\tat com.databricks.common.filesystem.LokiFileSystem.create(LokiFileSystem.scala:298)\\n\\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.create(CredentialScopeFileSystem.scala:220)\\n\\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1233)\\n\\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1210)\\n\\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1091)\\n\\tat com.databricks.sql.io.HDFSStorage.create(HDFSStorage.java:209)\\n\\tat com.databricks.sql.io.NativeStorage.create(NativeStorage.java:468)\\n\\tat com.databricks.photon.NativeDataWriter.openNewFile(NativeDataWriter.scala:155)\\n\\tat 0xb651ee7 <photon>.OpenNewFile(external/workspace_spark_3_5/photon/exec-nodes/data-writer.cc:77)\\n\\tat 0x6fcfc0b <photon>.OpenNewFileInCurrentPartition(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:434)\\n\\tat 0x6fd06bb <photon>.WriteBatch(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:350)\\n\\tat 0x6fce823 <photon>.operator()(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:201)\\n\\tat 0x6f4022f <photon>.ForEach(external/workspace_spark_3_5/photon/data-layout/column-batch-iterator.h:53)\\n\\tat 0x6f4022f <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-writer-node.cc:201)\\n\\tat com.databricks.photon.JniApiImpl.open(Native Method)\\n\\tat com.databricks.photon.JniApi.open(JniApi.scala)\\n\\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\\n\\tat com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$4(PhotonWriteStageExec.scala:128)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\\n\\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\\n\\tat com.databricks.photon.PhotonWriteResultHandler.timeit(PhotonWriteStageExec.scala:68)\\n\\tat com.databricks.photon.PhotonWriteResultHandler.$anonfun$getResult$3(PhotonWriteStageExec.scala:128)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1621)\\n\\tat com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:125)\\n\\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:237)\\n\\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\\n\\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.runFuncAsBillable(PhotonBasicEvaluatorFactory.scala:291)\\n\\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:234)\\n\\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)\\n\\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\\n\\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\\n\\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\\n\\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\\n\\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\\n\\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\\n\\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\\n\\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\\n\\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\\n\\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\\n\\tat scala.util.Using$.resource(Using.scala:269)\\n\\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\\n\\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\\n\\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$12(Executor.scala:1221)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1225)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\\n\\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1076)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\nDriver stacktrace:\"}}}",
    "headers" : {
      "x-request-id" : "dc5f5b3d-7390-471a-b7f6-421c2a9d559f",
      "date" : "Wed, 12 Feb 2025 04:23:21 GMT",
      "server" : "databricks",
      "x-databricks-popp-response-code-details" : "via_upstream",
      "x-databricks-shard-debug" : "oregon-staging",
      "vary" : "Accept-Encoding",
      "x-databricks-org-id" : "6051921418418893",
      "strict-transport-security" : "max-age=31536000; includeSubDomains; preload",
      "x-content-type-options" : "nosniff",
      "x-databricks-popp-routing-reason" : "deployment-name",
      "content-type" : "application/json",
      "x-databricks-apiproxy-response-code-details" : "via_upstream"
    }
  },
  "uuid" : "22cbf314-e84e-452f-865a-b16bf019d96b",
  "scenarioName" : "scenario-2-api-2.0-sql-statements-01efe8f8-e6a3-1b88-b331-ff4f5465bc03",
  "requiredScenarioState" : "scenario-2-api-2.0-sql-statements-01efe8f8-e6a3-1b88-b331-ff4f5465bc03-2",
  "insertionIndex" : 88
}