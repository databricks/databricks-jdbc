<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>ChunkExtractor.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">jacoco.exec</a> &gt; <a href="index.source.html" class="el_package">com.databricks.jdbc.core</a> &gt; <span class="el_source">ChunkExtractor.java</span></div><h1>ChunkExtractor.java</h1><pre class="source lang-java linenums">package com.databricks.jdbc.core;

import static com.databricks.jdbc.core.DatabricksTypeUtil.*;

import com.databricks.jdbc.client.impl.thrift.generated.*;
import com.databricks.jdbc.core.types.CompressionType;
import com.google.common.annotations.VisibleForTesting;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import org.apache.arrow.vector.types.pojo.ArrowType;
import org.apache.arrow.vector.types.pojo.Field;
import org.apache.arrow.vector.types.pojo.FieldType;
import org.apache.arrow.vector.types.pojo.Schema;
import org.apache.arrow.vector.util.SchemaUtility;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/** Class to manage inline Arrow chunks */
public class ChunkExtractor {

<span class="fc" id="L24">  private static final Logger LOGGER = LogManager.getLogger(ChunkExtractor.class);</span>
  private long totalRows;
  private long currentChunkIndex;
  private ByteArrayInputStream byteStream;

  ArrowResultChunk arrowResultChunk; // There is only one packet of data in case of inline arrow

  ChunkExtractor(List&lt;TSparkArrowBatch&gt; arrowBatches, TGetResultSetMetadataResp metadata)
<span class="fc" id="L32">      throws DatabricksParsingException {</span>
<span class="fc" id="L33">    this.currentChunkIndex = -1;</span>
<span class="fc" id="L34">    this.totalRows = 0;</span>
<span class="fc" id="L35">    initializeByteStream(arrowBatches, metadata);</span>
    // Todo : Add compression appropriately
<span class="fc" id="L37">    arrowResultChunk = new ArrowResultChunk(totalRows, null, CompressionType.NONE, byteStream);</span>
<span class="fc" id="L38">  }</span>

  public boolean hasNext() {
<span class="fc bfc" id="L41" title="All 2 branches covered.">    return this.currentChunkIndex == -1;</span>
  }

  public ArrowResultChunk next() {
<span class="fc bfc" id="L45" title="All 2 branches covered.">    if (this.currentChunkIndex != -1) {</span>
<span class="fc" id="L46">      return null;</span>
    }
<span class="fc" id="L48">    this.currentChunkIndex++;</span>
<span class="fc" id="L49">    return arrowResultChunk;</span>
  }

  private void initializeByteStream(
      List&lt;TSparkArrowBatch&gt; arrowBatches, TGetResultSetMetadataResp metadata)
      throws DatabricksParsingException {
<span class="fc" id="L55">    ByteArrayOutputStream baos = new ByteArrayOutputStream();</span>
    try {
<span class="fc" id="L57">      byte[] serializedSchema = getSerializedSchema(metadata);</span>
<span class="pc bpc" id="L58" title="1 of 2 branches missed.">      if (serializedSchema != null) {</span>
<span class="fc" id="L59">        baos.write(serializedSchema);</span>
      }
<span class="fc bfc" id="L61" title="All 2 branches covered.">      for (TSparkArrowBatch arrowBatch : arrowBatches) {</span>
<span class="fc" id="L62">        totalRows += arrowBatch.getRowCount();</span>
<span class="fc" id="L63">        baos.write(arrowBatch.getBatch());</span>
<span class="fc" id="L64">      }</span>
<span class="fc" id="L65">      this.byteStream = new ByteArrayInputStream(baos.toByteArray());</span>
<span class="nc" id="L66">    } catch (DatabricksSQLException | IOException e) {</span>
<span class="nc" id="L67">      handleError(e);</span>
<span class="fc" id="L68">    }</span>
<span class="fc" id="L69">  }</span>

  private byte[] getSerializedSchema(TGetResultSetMetadataResp metadata)
      throws DatabricksSQLException {
<span class="pc bpc" id="L73" title="1 of 2 branches missed.">    if (metadata.getArrowSchema() != null) {</span>
<span class="nc" id="L74">      return metadata.getArrowSchema();</span>
    }
<span class="fc" id="L76">    Schema arrowSchema = hiveSchemaToArrowSchema(metadata.getSchema());</span>
    try {
<span class="fc" id="L78">      return SchemaUtility.serialize(arrowSchema);</span>
<span class="nc" id="L79">    } catch (IOException e) {</span>
<span class="nc" id="L80">      handleError(e);</span>
    }
    // should never reach here;
<span class="nc" id="L83">    return null;</span>
  }

  private static Schema hiveSchemaToArrowSchema(TTableSchema hiveSchema)
      throws DatabricksParsingException {
<span class="fc" id="L88">    List&lt;Field&gt; fields = new ArrayList&lt;&gt;();</span>
<span class="fc bfc" id="L89" title="All 2 branches covered.">    if (hiveSchema == null) {</span>
<span class="fc" id="L90">      return new Schema(fields);</span>
    }
    try {
<span class="fc" id="L93">      hiveSchema</span>
<span class="fc" id="L94">          .getColumns()</span>
<span class="fc" id="L95">          .forEach(</span>
              columnDesc -&gt; {
                try {
<span class="fc" id="L98">                  fields.add(getArrowField(columnDesc));</span>
<span class="nc" id="L99">                } catch (DatabricksSQLException e) {</span>
<span class="nc" id="L100">                  throw new RuntimeException(e);</span>
<span class="fc" id="L101">                }</span>
<span class="fc" id="L102">              });</span>
<span class="nc" id="L103">    } catch (RuntimeException e) {</span>
<span class="nc" id="L104">      handleError(e);</span>
<span class="fc" id="L105">    }</span>
<span class="fc" id="L106">    return new Schema(fields);</span>
  }

  private static Field getArrowField(TColumnDesc columnDesc) throws DatabricksSQLException {
<span class="fc" id="L110">    TTypeId thriftType = getThriftTypeFromTypeDesc(columnDesc.getTypeDesc());</span>
<span class="fc" id="L111">    ArrowType arrowType = null;</span>
<span class="fc" id="L112">    arrowType = mapThriftToArrowType(thriftType);</span>
<span class="fc" id="L113">    FieldType fieldType = new FieldType(true, arrowType, null);</span>
<span class="fc" id="L114">    return new Field(columnDesc.getColumnName(), fieldType, null);</span>
  }

  @VisibleForTesting
  static void handleError(Exception e) throws DatabricksParsingException {
<span class="fc" id="L119">    String errorMessage = &quot;Cannot process inline arrow format. Error: &quot; + e.getMessage();</span>
<span class="fc" id="L120">    LOGGER.error(errorMessage);</span>
<span class="fc" id="L121">    throw new DatabricksParsingException(errorMessage, e);</span>
  }

  public void releaseChunk() {
<span class="fc" id="L125">    this.arrowResultChunk.releaseChunk();</span>
<span class="fc" id="L126">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>